## On Designing and Deploying Internet-Scale Services


### 简介

本文介绍了设计与部署大规模可靠服务的各类最佳实践，从设计、实现、测试、部署、运维，总结了服务稳定需要的方方面面，是一篇很全面的综述性文章。
本文旨在指导开发者编写 “运维友好” 的服务，来提升服务稳定性，减少人工介入成本。

### 正文

#### 前提

1. 失败预期：任何东西都有可能出问题，应提前做好准备；
2. 简单化：简单的东西好维护；
3. 自动化：尽量让机器自动执行，减少人工介入成本；

#### 设计

设计是最重要的阶段，好的设计能有效地减少服务的维护成本。
「标准化」（也就是文中的 constrained service model）是自动化的前提。高效、自动，甚至将来的「智能」都离不开标准化。

1. 面向失败设计服务：依赖都是不可靠的，能自动恢复的故障应避免人工干预；
2. 冗余与故障恢复：通常通过多实例分布式集群与自动化的故障摘除来实现。在当前云时代，这是基本要求与基本保障；
3. 硬件分片：硬件应与服务类型匹配；
4. 单一版本软件：避免同时维护多个版本，将控制权掌控在服务端，保持升级时的后向兼容；
5. 多租户；类似软件的单一版本，使用同一套环境为多租户提供服务，避免部署、运维操作上的复杂性；

运维友好服务的设计
1. 服务简单可验证；
2. 做好限流、熔断；
3. 了解底层系统、网络环境；
4. 弄清请求与资源的关系；


#### 依赖管理
1. 超时管理：请求依赖服务时，设置合理的超时时间；
2. 故障隔离：避免依赖组件故障影响自身服务；
3. 依赖成熟度：使用成熟的、可靠的依赖组件；
4. 监控报警：监控依赖组件并配置报警；
5. 依赖解耦：区分强弱依赖，故障时降级弱依赖，保持服务可用；


#### 发布与测试
1. 频繁发布：小步快跑，持续迭代，避免大版本发布带来的过多 bug；
2. 服务压测：发现服务容量上限，方便容量评估、限流等；
3. 发布测试：发布前进行功能及性能测试；
4. 真实数据：复制线上流量，进行高仿真度测试，效果较好；


#### 硬件标准化
1. 套餐标准化：根据服务场景创建少量标准硬件套餐，是硬件标准化的一种方案；
2. 网络命名抽象：上层业务使用 CNAME 配置，方便故障切换；


#### 容量预估
1. 资源分配管理：通过压测的方式了解流量与后端资源的关系，数据可用于运维侧的容量规划；


#### 监控报警
1. 监控上报：主动上报业务指标，并针对做报警；
2. 主动探测：从用户角度主动探测服务可用性；
3. 延时分位值：通过分位值监控延时数据，效果较好；
4. 记录生产环境数据：
    1. 操作审计数据记录；
    2. 故障自愈数据记录；
5. 可配置的日志：需要 debug 时，修改配置快速实现，避免上线；
6. 健康监控：一般有：端口监控、进程监控、语义监控三种；
7. 有动作的报警：收到报警一定是要有所行动的，无需操作的报警，通常需要删除、屏蔽或调整报警阈值；


#### 限流降级
1. 切流限流：在服务入口层做限流、切流；极端故障可通过入口限流到底，再逐步放开实现业务恢复；
2. 熔断降级：保证第三方依赖的可降级性，防止其故障拖垮自身；


### 总结
整体思路与《Google SRE》相似，简短、综述式地描绘了如何构建稳定可靠服务的方方面面。

文章发表于 2007 年，很多内容在当前视角来看，已经是常识。作者在十几年前已经将软件稳定性思路总结的如此全面，实属不易。

个人而言，重新复习了一次稳定性建设周边；后续应该多读读业界先进论文，多刷新知识，加油。
