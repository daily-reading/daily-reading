# Modern storage is plenty fast. It is the APIs that are bad.

* Author: Glauber Costa
* Link: https://itnext.io/modern-storage-is-plenty-fast-it-is-the-apis-that-are-bad-6a68319fbc1a

## 为什么 I/O 操作需要缓存处理

传统的 Linux I/O API 需要读取未缓存在内存中的数据时，会通过缺页中断的, 然后数据准备好之后，再恢复。对于传统的基于系统调用的读取，你要额外拷贝一份到用户缓冲区，对于基于mmap的操作，你要更新虚拟内存映射。

实际上 产生缺页中断、拷贝内存或虚拟内存映射更新都不便宜。但在多年前，它们的成本还是比I/O本身的成本便宜约100倍，因此这种方法是可以接受的。随着设备延迟接近微秒级，这种情况已经不再。这些操作的延迟现在与 I/O 操作本身是同一个数量级的。

而实际上，一次 I/O 的总开销里，只有不到一半是与磁盘设备本身的通信开销。

## 读放大问题

作者认为，现代NVMe支持多并发操作，因此多文件读取不一定比单文件读取数据开销更大。举个例子:

因为 Linux 中内存页大小为 4kB，这意味着它每次只能以最低4kB的速度读取，超过就要做换页处理。也就是说如果你需要将1KB 的数据分两个文件读取，每个文件512字节，那么你实际上是在读取 8kB 来服务1kB，浪费了87%的数据读取量。但在实际操作中更夸张，因为 OS 会做 read-ahead，默认设置为128kB, 但如果你用不了这么多预读 buffer 的话，就会造成用了 256KB 来读取 1KB，实际上浪费了99%的数据。

> 在试验中，从多个文件中读取数据不应该从根本上比从单个文件中读取数据慢，是因为读取放大后，有效读取的数据量增加了很多(数据处理的开销增大)

## 传统 API 没有并行能力

跳过 OS 页缓存, Direct IO 也不会更快，原因是 **传统的 I/O API 没有并行能力**。一个文件被看作是一个连续的字节流, 数据是否在内存中对用户是透明的。传统的API会等到你接触到非驻留的数据时才会发出I/O操作。虽然现代磁盘设备的速度很快，但仍然比CPU慢。当设备在等待I/O操作回来的时候，CPU其实什么也不做。

使用多文件的话，会有以下问题:
- 多个文件意味着多个 read-ahead 缓冲区，增加了随机I/O的浪费因素。
- 在 thread-poll 的 API 设计中，多个文件意味着多个线程，每次 IO 操作都额外增加了线程上下文切换的开销。


## 基于 io_uring 封装的全新 API

io_uring 很好，但是有一些问题:

- 通过io_uring发送的I/O如果使用 bufferd files，仍然会受到前面列出的大部分问题的影响.
- 使用 Direct I/O 的话坑比较多，比如内存必需对齐，还有记录读取的位置等.


于是作者设计一个 Rust 的文件 I/O 库, ![Glommio](https://github.com/DataDog/glommio), 主要是针对 Direct I/O 做的一些优化，支持一些高级特性，例如缓冲区注册和基于轮询的无中断方式来实现 Direct I/O。这个库支持两种你那个类型的文件，**随机访问型文件**和**流式文件**. API 层的优化也主要是针对 **随机访问型文件**，主要是用到了 io_uring 的 ring buffer，使用固定的缓冲区大小来节省内存，每次读取完一个缓冲区该缓冲区就会立刻释放。据作者测试，性能大概能提升7倍左右。详细的 API 设计可以看 ![rust-doc](https://docs.rs/glommio/0.2.0-alpha/glommio/)
